{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code first time\n",
    "\n",
    "#!pip3 install openai\n",
    "#!pip3 install requests\n",
    "#!pip3 install numpy\n",
    "#!pip3 install torch\n",
    "#!pip3 install torchvision\n",
    "#!pip3 install transformers\n",
    "#!pip3 install span-marker\n",
    "#!pip3 install langdetect\n",
    "#!pip3 install regex  \n",
    "#!pip3 install pickle5  \n",
    "#!pip3 install spacy-langdetect  \n",
    "#!pip3 install langdetect\n",
    "#!pip3 install python-dotenv\n",
    "#!pip3 install SPARQLWrapper\n",
    "#!pip3 install urllib3\n",
    "#!pip3 install py2neo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b0a798",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 19:07:59.934051: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from span_marker import SpanMarkerModel\n",
    "from langdetect import detect\n",
    "from langdetect import LangDetectException\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import urllib.error\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d297b4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variables\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc59a5",
   "metadata": {},
   "source": [
    "Initialize Entity Categories and Relation Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5423cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories = [\n",
    "    \n",
    "    \"Person\",\n",
    "    \"Location\",\n",
    "    \"Organization\",\n",
    "    \"Event\",\n",
    "    \"Product\",\n",
    "    \"Project\",\n",
    "    \"Skill\",\n",
    "    \"Strategy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa069f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_labels = [\n",
    "    \"implements\",\n",
    "    \"funds\",\n",
    "    \"focuses_on\",\n",
    "    \"in\",\n",
    "    \"partners_with\",\n",
    "    \"contributes_to\",\n",
    "    \"monitors\",\n",
    "    \"targets\",\n",
    "    \"addresses\",\n",
    "    \"employs\",\n",
    "    \"collaborates_with\",\n",
    "    \"supports\",\n",
    "    \"administers\",\n",
    "    \"measures\",\n",
    "    \"aligns_with\",\n",
    "    \"an_instance_of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5f1a1",
   "metadata": {},
   "source": [
    "# Setting up OpenAI connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a472f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "neo4j_pass = os.getenv(\"NEO4JPASS\")\n",
    "#openai.api_key = os.getenv(\"OPENAI_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbda6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"OPENAI_API_KEY\"),  \n",
    "  api_version = os.getenv(\"OPENAI_API_VERSION\"),\n",
    "  azure_endpoint =os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76acd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get answers from older version of openAI\n",
    "\n",
    "def get_answer_old(user_question, timeout_seconds):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"sdgi-gpt-35-turbo-16k\", \n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            request_timeout = timeout_seconds\n",
    "            # max_tokens=2000\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "    except requests.Timeout:\n",
    "        print(f\"Request timed out\")\n",
    "        return []\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ded71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get answers from GPT\n",
    "\n",
    "def get_answer(prompt):  \n",
    "    response_entities = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ]\n",
    "                )\n",
    "    response = response_entities.choices[0].message.content\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e328a35",
   "metadata": {},
   "source": [
    "# Entity Extraction using Transformers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f369316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting with two transformer models using API Inference\n",
    "\n",
    "WIKI_API = \"https://api-inference.huggingface.co/models/Babelscape/wikineural-multilingual-ner\"\n",
    "BERT_API = \"https://api-inference.huggingface.co/models/dslim/bert-base-NER\"\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "#functions to get responses from the above models\n",
    "\n",
    "def query_wiki(payload):\n",
    "\tresponse = requests.post(WIKI_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "\n",
    "def query_bert(payload):\n",
    "\tresponse = requests.post(BERT_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb268621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract acronyms from text using GPT prompt\n",
    "\n",
    "def query_gpt(text):\n",
    "    \n",
    "    entities_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<. You have two tasks:\n",
    "    \n",
    "    1. Your first task is to detect acronyms with their names and store them in python dictionary.\n",
    "    2. Your second task is to detect Proper Nouns in the text and store them in python list.\n",
    "    \n",
    "    Return a JSON array contaning dictionary and the list.\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    #start_time = time.time()\n",
    "\n",
    "    result = get_answer(entities_prompt)\n",
    "    result = json.loads(result)\n",
    "    \n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = end_time - start_time\n",
    "    #print (f\"TIME TAKEN TO EXECUTE PROMPT: {elapsed_time}\")\n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4160364",
   "metadata": {},
   "source": [
    "# Text Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a13f4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to clean and pre-process data\n",
    "\n",
    "def split_text_spacy(chunk_size, text):\n",
    "    \n",
    "    text_splitter = SpacyTextSplitter(chunk_size=chunk_size)\n",
    "    sections = text_splitter.split_text(text)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "def get_text_section(limit, text):\n",
    "    sections_list = []\n",
    "    length = len(text)\n",
    "    i = 0\n",
    "\n",
    "    while i < length - 1:\n",
    "        j = i + limit\n",
    "\n",
    "        if j >= length:\n",
    "            j = length - 1\n",
    "        elif text[j] not in ('.', '\\n', ';'):\n",
    "            while text[j] not in ('.', '\\n', ';'):\n",
    "                j -= 1\n",
    "            j += 1\n",
    "\n",
    "        section = text[i:j]\n",
    "\n",
    "        if is_valid_section(section):\n",
    "            sections_list.append(section)\n",
    "        else: \n",
    "            print(\"INVALID SECTION DETECTED\")\n",
    "            print(section)\n",
    "            #section_list[-1].extend(section)\n",
    "        i = j\n",
    "    \n",
    "    \n",
    "    return sections_list\n",
    "\n",
    "def is_valid_section(section):\n",
    "    return section and len(section) > 20\n",
    "\n",
    "\n",
    "def clean_text(input_text):\n",
    "    # Remove lines with only whitespace\n",
    "    input_text = re.sub(r'^\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines containing only uppercase text (potential headings)\n",
    "    input_text = re.sub(r'^\\s*[A-Z\\s]+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines with multiple consecutive uppercase words (potential headings)\n",
    "    input_text = re.sub(r'^\\s*(?:[A-Z]+\\s*){2,}\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "    \n",
    "    input_text = re.sub(r'^\\s*[A-Za-z\\s]+\\.{3,}\\s*\\d+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    return input_text\n",
    "\n",
    "def is_english(line):\n",
    "    try:\n",
    "        return detect(line) == 'en'\n",
    "    except LangDetectException as e:\n",
    "        print(f\"An exception occurred: {e} : {line}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38eb33c",
   "metadata": {},
   "source": [
    "# Functions to process extracted entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec7e10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the broken entities recieved by transformers to needed format\n",
    "\n",
    "def create_entities(lst):\n",
    "    i = 1\n",
    "    while i < len(lst):\n",
    "        if lst[i][\"word\"].startswith('##'):\n",
    "            lst[i][\"word\"] = lst[i-1][\"word\"] + lst[i][\"word\"][2:]\n",
    "            lst[i][\"score\"] = max(lst[i-1][\"score\"] , lst[i][\"score\"])\n",
    "            del lst[i-1]\n",
    "        else:\n",
    "            i += 1\n",
    "            \n",
    "\n",
    "# threshold score to eliminate unimportant entities\n",
    "def apply_threshold(list_, threshold):\n",
    "    words_list = []\n",
    "    for item in list_:\n",
    "        if item['score'] > threshold:  \n",
    "            words_list.append(item['word'])\n",
    "    return words_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f27e0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get raw version of entities for comparison\n",
    "def get_raw(list_):\n",
    "    output = []\n",
    "    for sublist in list_:\n",
    "        new = []\n",
    "        obj = {}\n",
    "        for item in sublist:\n",
    "            #obj = {}\n",
    "            key = ''.join(filter(str.isalpha, item))\n",
    "            obj[key]= item\n",
    "            #obj['raw']= ''.join(filter(str.isalpha, item))\n",
    "        output.append(obj)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e749549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find intersection of entities found from transformers \n",
    "\n",
    "def merge_extracted_entities(wiki, bert, gpt):\n",
    "    \n",
    "    output = set(wiki.values())\n",
    "    dict_ = gpt\n",
    "    \n",
    "    bert_set = set(bert.keys()) - set(wiki.keys())\n",
    "    gpt_set = set(gpt.keys())\n",
    "    \n",
    "    A = gpt_set.intersection(bert_set)\n",
    "\n",
    "    matched = list(set(A))\n",
    "    print (\"GPT/BERT: \" + str(matched))\n",
    "\n",
    "    for i in matched:\n",
    "        output.add(dict_[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0085ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_extracted_entities_old(wiki, bert, gpt):\n",
    "    \n",
    "    output = []\n",
    "    dict_ = gpt\n",
    "    \n",
    "    \n",
    "    #print(dict_.items())\n",
    "    wiki_set = set(wiki.keys())\n",
    "    #bert_set = set(bert.keys())\n",
    "    gpt_set = set(gpt.keys())\n",
    "    \n",
    "    \n",
    "    #A = gpt_set.intersection(bert_set)\n",
    "    #B = bert_set.intersection(wiki_set)\n",
    "    C = wiki_set.intersection(gpt_set)\n",
    " \n",
    "    #matched = list(A.union(B).union(C))\n",
    "    \n",
    "    \n",
    "    for i in C:\n",
    "        output.append(dict_[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ff671cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove any unwanted characters\n",
    "def validate_entities(list_):\n",
    "    \n",
    "    # Define a regular expression pattern to match invalid characters.\n",
    "    pattern = r'\\s*{}\\s*'.format(re.escape(\"’\"))\n",
    "    pattern1 = r'\\s*{}\\s*'.format(re.escape(\"/\"))\n",
    "    output_list = []\n",
    "\n",
    "    for item in list_:\n",
    "        item = re.sub(pattern, \"’\", item)\n",
    "        tem = re.sub(pattern1, \"/\", item)\n",
    "            \n",
    "    return output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6c56a",
   "metadata": {},
   "source": [
    "#  Categorize entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "197cdc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero shot prompt to categorize entities\n",
    "\n",
    "def categorize_entities(text, entities, categories):\n",
    "    \n",
    "    \n",
    "    categorization_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and >>>>>Categories<<<<<. \n",
    "    Your task is to assign a sutiable category to each element of >>>>>EntityList<<<<<.\n",
    "    \n",
    "    Return a list of JSON objects of categorized entities. \n",
    "\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>Categories<<<<<\n",
    "    {categories}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "    \"\"\"\n",
    "\n",
    "    categorized_entities = get_answer(categorization_prompt)\n",
    "    categorized_entities = json.loads(categorized_entities)\n",
    "    \n",
    "    return (categorized_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d1e03",
   "metadata": {},
   "source": [
    "# Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87999db",
   "metadata": {},
   "source": [
    "Two approaches were tried to extract relations. Finally the extract_ontology_relations was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ffc4f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ontology.ttl\") as f:\n",
    "    ontology = f.read()\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ebf0d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero shot prompt to extract relations using relation labels\n",
    "\n",
    "\n",
    "def extract_relation_details(text, entities, relation_labels):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and a list of >>>>>RelationLabels<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between elements of provided >>>>>EntityList<<<<<.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and store the >>>>>RelationLabels<<<<<.\n",
    "    2. Scan the >>>>>TEXT<<<<< to find Named Entites from >>>>>EntityList<<<<< that are related.\n",
    "    3. Scan the >>>>>RelationLabels<<<<< to select a suitable label to\n",
    "    describe the relation between the above selected entities. Mark this label as \"Relation\".\n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the selected \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should belong to >>>>>RelationLabels<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>RelationLabels<<<<<\n",
    "    {relation_labels}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "513a5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero shot prompt to extract relations using ontology\n",
    "\n",
    "\n",
    "def extract_ontology_relations(text, entities, ontology):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and an >>>>>ONTOLOGY<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between elements of provided >>>>>EntityList<<<<<. Use the given >>>>>ONTOLOGY<<<<<\n",
    "   for this purpose.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and understand the >>>>>ONTOLOGY<<<<<.\n",
    "    2. Scan the >>>>>TEXT<<<<< to find Named Entites in >>>>>EntityList<<<<< that are related.\n",
    "    3. Read the >>>>>ONTOLOGY<<<<< to select a relationship type for the related entities. Mark this label as \"Relation\".\n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should be a label from properties in >>>>>ONTOLOGY<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>ONTOLOGY<<<<<\n",
    "    {ontology}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862fd69",
   "metadata": {},
   "source": [
    "# Functions to get knowledge from Dbpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f2dfa",
   "metadata": {},
   "source": [
    "Used for entity enrichment. Provides summaries for entities found in DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a2a53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the DBpedia SPARQL endpoint\n",
    "sparql_endpoint = \"http://dbpedia.org/sparql\"\n",
    "\n",
    "# Create a SPARQLWrapper instance\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "\n",
    "# Function to search for an entity by label and return its DBpedia URI\n",
    "def search_entity(label):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?entity\n",
    "    WHERE {{\n",
    "      ?entity rdfs:label \"{label}\"@en.\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in results and \"bindings\" in results[\"results\"] and results[\"results\"][\"bindings\"]:\n",
    "        entity_uri = results[\"results\"][\"bindings\"][0][\"entity\"][\"value\"]\n",
    "        return entity_uri\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to retrieve and return the abstract or comment of an entity\n",
    "def retrieve_entity_summary(entity_uri):\n",
    "    # Try to retrieve the abstract\n",
    "    abstract_query = f\"\"\"\n",
    "    SELECT ?abstract\n",
    "    WHERE {{\n",
    "      <{entity_uri}> dbo:abstract ?abstract.\n",
    "      FILTER (LANGMATCHES(LANG(?abstract), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(abstract_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    abstract_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in abstract_results and \"bindings\" in abstract_results[\"results\"]:\n",
    "        for result in abstract_results[\"results\"][\"bindings\"]:\n",
    "            abstract = result[\"abstract\"][\"value\"]\n",
    "            return abstract\n",
    "\n",
    "    # If abstract is not found, try to retrieve the comment\n",
    "    comment_query = f\"\"\"\n",
    "    SELECT ?comment\n",
    "    WHERE {{\n",
    "      <{entity_uri}> rdfs:comment ?comment.\n",
    "      FILTER (LANGMATCHES(LANG(?comment), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(comment_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    comment_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in comment_results and \"bindings\" in comment_results[\"results\"]:\n",
    "        for result in comment_results[\"results\"][\"bindings\"]:\n",
    "            comment = result[\"comment\"][\"value\"]\n",
    "            return comment\n",
    "\n",
    "    # If neither abstract nor comment is found, return None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7c57444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dbpedia_summary(search_label):\n",
    "    entity_uri = search_entity(search_label)\n",
    "\n",
    "    if entity_uri:\n",
    "        print(f\"Entity found with DBpedia URI: {entity_uri}\")\n",
    "        try:\n",
    "            summary = retrieve_entity_summary(entity_uri)\n",
    "            if summary:\n",
    "                return summary\n",
    "            else:\n",
    "                print(\"No abstract or comment found for this entity.\")\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(f\"No entity found with the label: {search_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5be620ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summaries(entities):\n",
    "    \n",
    "    updated_entities = []\n",
    "\n",
    "    for item in entities:\n",
    "        try:\n",
    "            summary = dbpedia_summary(item['entity'])\n",
    "            if summary:\n",
    "                # Only add the summary if it's not None or empty\n",
    "                item['summary'] = summary\n",
    "            updated_entities.append(item)  # Add the item regardless of summary presence\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return updated_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7975f7",
   "metadata": {},
   "source": [
    "# Creating Graph in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f98e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add user and password of database on your Neo4j \n",
    "graph = Graph(uri = 'bolt://localhost:7687',user='neo4j',password=neo4j_pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047b2ca",
   "metadata": {},
   "source": [
    "Functions to add entities and relations to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d2cd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Document:\n",
    "    def __init__(self, metadata, entities, relations):\n",
    "        self.metadata = metadata\n",
    "        self.entities = entities\n",
    "        self.relations = relations\n",
    "\n",
    "# Define a function to create or retrieve a node\n",
    "def get_or_create_node(label, key, value):\n",
    "    # Attempt to find an existing node with the given label and key\n",
    "    existing_node = get_node(label, key, value)\n",
    "    if not existing_node:\n",
    "        existing_node = get_node(label, 'acronym', value)\n",
    "    \n",
    "    if existing_node:\n",
    "        return existing_node\n",
    "    else:\n",
    "        new_node = Node(label, **{key: value})\n",
    "        graph.create(new_node)\n",
    "        return new_node\n",
    "    \n",
    "def get_node(label, key, value):\n",
    "    node = graph.nodes.match(label, **{key:value}).first()\n",
    "    return node\n",
    "def get_node_without_label(key, value):\n",
    "    node = graph.nodes.match(**{key:value}).first()\n",
    "    return node\n",
    "\n",
    "# Define a function to insert relations \n",
    "def insert_relations_neo4j(entities, relations):\n",
    "    \n",
    "    for item in entities:\n",
    "        \n",
    "        node = get_or_create_node('Entity', \"name\", item[\"entity\"])\n",
    "        node['category'] = item[\"category\"]\n",
    "        if \"acronym\" in item:\n",
    "            node['acronym'] = item[\"acronym\"]\n",
    "        if \"summary\" in item:\n",
    "            node['summary'] = item[\"summary\"]\n",
    "        \n",
    "        graph.push(node)\n",
    "        \n",
    "        \n",
    "    for item in relations:\n",
    "        subject = get_or_create_node( \"Entity\", \"name\", item[\"Subject\"])\n",
    "        obj = get_or_create_node(\"Entity\", \"name\", item[\"Object\"])\n",
    "        relation = Relationship(subject, item[\"Relation\"], obj)\n",
    "        if 'Description' in item:\n",
    "            relation[\"description\"] = item[\"Description\"]\n",
    "        graph.create(relation)\n",
    "        \n",
    "    \n",
    "# Define a function to insert summaries \n",
    "def insert_summary_neo4j(data):\n",
    "    for item in data:\n",
    "        node = get_node(\"Entity\", \"name\", item.name)\n",
    "        node[\"Summary\"] = item.summary\n",
    "        graph.push(node)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af30def",
   "metadata": {},
   "source": [
    "# Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7cd35416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 6\n",
      "\n",
      "['AFG-NRER-2017-EN.txt', 'AFG-CPD-2014-EN.txt', 'AFG-NEP-2015-EN.txt', 'AFG-NEPro-2022-EN.txt', 'AFG-NREP-2013-EN.txt', 'AFG-NREP-2015-EN.txt']\n"
     ]
    }
   ],
   "source": [
    "#read the files from Data folder\n",
    "folder_path = ('Data/AFG')\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter the list to include only text files (e.g., .txt files)\n",
    "text_files = [file for file in file_list if file.endswith(\".txt\")]\n",
    "\n",
    "print (f\"Number of files: {len(text_files)}\\n\")  \n",
    "print (text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "644ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the file \n",
    "file_path = os.path.join(folder_path, text_files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "32344bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 7562\n"
     ]
    }
   ],
   "source": [
    "with open (file_path, 'r') as file:\n",
    "    head = [next(file) for _ in range(11)]\n",
    "    next(file)\n",
    "    raw_text = file.read()\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "print (f\"Original text length: {len(raw_text)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a1d8fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: No features in text. : \n",
      "\n",
      "An exception occurred: No features in text. : 56%\n",
      "\n",
      "An exception occurred: No features in text. : 2%\n",
      "\n",
      "An exception occurred: No features in text. : 22%\n",
      "\n",
      "An exception occurred: No features in text. : 16%1% 0%\n",
      "\n",
      "An exception occurred: No features in text. : 83%\n",
      "\n",
      "An exception occurred: No features in text. : 98%\n",
      "\n",
      "An exception occurred: No features in text. : 33%\n",
      "\n",
      "An exception occurred: No features in text. : 19%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 20%\n",
      "\n",
      "An exception occurred: No features in text. : 40%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : 75.6 74.3 74.5 74.9\n",
      "\n",
      "An exception occurred: No features in text. : 82.5\n",
      "\n",
      "An exception occurred: No features in text. : 69.4\n",
      "\n",
      "An exception occurred: No features in text. : 0\n",
      "\n",
      "An exception occurred: No features in text. : 10\n",
      "\n",
      "An exception occurred: No features in text. : 20\n",
      "\n",
      "An exception occurred: No features in text. : 30\n",
      "\n",
      "An exception occurred: No features in text. : 40\n",
      "\n",
      "An exception occurred: No features in text. : 50\n",
      "\n",
      "An exception occurred: No features in text. : 60\n",
      "\n",
      "An exception occurred: No features in text. : 70\n",
      "\n",
      "An exception occurred: No features in text. : 80\n",
      "\n",
      "An exception occurred: No features in text. : 90\n",
      "\n",
      "An exception occurred: No features in text. : 100\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019\n",
      "\n",
      "An exception occurred: No features in text. : )\n",
      "\n",
      "An exception occurred: No features in text. : -4%\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.5\n",
      "\n",
      "An exception occurred: No features in text. : 1.0\n",
      "\n",
      "An exception occurred: No features in text. : 1.5\n",
      "\n",
      "An exception occurred: No features in text. : 2.0\n",
      "\n",
      "An exception occurred: No features in text. : 2.5\n",
      "\n",
      "An exception occurred: No features in text. : -5%\n",
      "\n",
      "An exception occurred: No features in text. : -4%\n",
      "\n",
      "An exception occurred: No features in text. : -3%\n",
      "\n",
      "An exception occurred: No features in text. : -2%\n",
      "\n",
      "An exception occurred: No features in text. : -1%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 1%\n",
      "\n",
      "An exception occurred: No features in text. : 2%\n",
      "\n",
      "An exception occurred: No features in text. : 5\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. :  20\n",
      "\n",
      "An exception occurred: No features in text. :  30\n",
      "\n",
      "An exception occurred: No features in text. :  40\n",
      "\n",
      "An exception occurred: No features in text. :  50\n",
      "\n",
      "An exception occurred: No features in text. :  60\n",
      "\n",
      "An exception occurred: No features in text. :  70\n",
      "\n",
      "An exception occurred: No features in text. :  80\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019\n",
      "\n",
      "An exception occurred: No features in text. : 2.4\n",
      "\n",
      "An exception occurred: No features in text. : 2.1\n",
      "\n",
      "An exception occurred: No features in text. : 2.2\n",
      "\n",
      "An exception occurred: No features in text. : 2.2\n",
      "\n",
      "An exception occurred: No features in text. : 2.3\n",
      "\n",
      "An exception occurred: No features in text. : 2.3\n",
      "\n",
      "An exception occurred: No features in text. : 2.4\n",
      "\n",
      "An exception occurred: No features in text. : 2.4\n",
      "\n",
      "An exception occurred: No features in text. : 2.5\n",
      "\n",
      "An exception occurred: No features in text. : 2.5\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019\n",
      "\n",
      "An exception occurred: No features in text. : 9.3\n",
      "\n",
      "An exception occurred: No features in text. : 8.2\n",
      "\n",
      "An exception occurred: No features in text. : 8.4\n",
      "\n",
      "An exception occurred: No features in text. : 8.6\n",
      "\n",
      "An exception occurred: No features in text. : 8.8\n",
      "\n",
      "An exception occurred: No features in text. : 9.0\n",
      "\n",
      "An exception occurred: No features in text. : 9.2\n",
      "\n",
      "An exception occurred: No features in text. : 9.4\n",
      "\n",
      "An exception occurred: No features in text. : 9.6\n",
      "\n",
      "An exception occurred: No features in text. : 9.8\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : + 8\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. : 14%\n",
      "\n",
      "An exception occurred: No features in text. : 74%\n",
      "\n",
      "An exception occurred: No features in text. : 11%\n",
      "\n",
      "An exception occurred: No features in text. : 0.0 0.0 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.1\n",
      "\n",
      "An exception occurred: No features in text. : 2016 2017 2018 2019 2020 2021\n",
      "\n",
      "An exception occurred: No features in text. : 4%\n",
      "\n",
      "An exception occurred: No features in text. : 23%\n",
      "\n",
      "An exception occurred: No features in text. : 32%\n",
      "\n",
      "An exception occurred: No features in text. : 5%\n",
      "\n",
      "An exception occurred: No features in text. : 0 20 40 60 80 100\n",
      "\n",
      "An exception occurred: No features in text. : 92%\n",
      "\n",
      "An exception occurred: No features in text. : 8%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 26 27\n",
      "\n",
      "An exception occurred: No features in text. : 29 29\n",
      "\n",
      "An exception occurred: No features in text. : 31 30\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  5\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. :  15\n",
      "\n",
      "An exception occurred: No features in text. :  20\n",
      "\n",
      "An exception occurred: No features in text. :  25\n",
      "\n",
      "An exception occurred: No features in text. :  30\n",
      "\n",
      "An exception occurred: No features in text. :  35\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019\n",
      "\n",
      "An exception occurred: No features in text. : 0.5 0.5\n",
      "\n",
      "An exception occurred: No features in text. : 0.6 0.6 0.6\n",
      "\n",
      "An exception occurred: No features in text. : 0.6 0.6 0.6\n",
      "\n",
      "An exception occurred: No features in text. : 57%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 20%\n",
      "\n",
      "An exception occurred: No features in text. : 40%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  1\n",
      "\n",
      "An exception occurred: No features in text. :  1\n",
      "\n",
      "An exception occurred: No features in text. :  1\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019 2020 2021\n",
      "\n",
      "An exception occurred: No features in text. : 54%\n",
      "\n",
      "An exception occurred: No features in text. : 1\n",
      "\n",
      "An exception occurred: No features in text. : 2\n",
      "\n",
      "An exception occurred: No features in text. : 3\n",
      "\n",
      "An exception occurred: No features in text. : 4\n",
      "\n",
      "An exception occurred: No features in text. : 5\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. :  20\n",
      "\n",
      "An exception occurred: No features in text. :  30\n",
      "\n",
      "An exception occurred: No features in text. :  40\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. :  2\n",
      "\n",
      "An exception occurred: No features in text. :  4\n",
      "\n",
      "An exception occurred: No features in text. :  6\n",
      "\n",
      "An exception occurred: No features in text. :  8\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. :  12\n",
      "\n",
      "An exception occurred: No features in text. :  14\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : 47%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 53%\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. :  20\n",
      "\n",
      "An exception occurred: No features in text. :  30\n",
      "\n",
      "An exception occurred: No features in text. :  40\n",
      "\n",
      "An exception occurred: No features in text. :  50\n",
      "\n",
      "An exception occurred: No features in text. :  60\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : -10%\n",
      "\n",
      "An exception occurred: No features in text. : 2,197\n",
      "\n",
      "An exception occurred: No features in text. : 5,322\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 1 000\n",
      "\n",
      "An exception occurred: No features in text. : 2 000\n",
      "\n",
      "An exception occurred: No features in text. : 3 000\n",
      "\n",
      "An exception occurred: No features in text. : 4 000\n",
      "\n",
      "An exception occurred: No features in text. : 5 000\n",
      "\n",
      "An exception occurred: No features in text. : 6 000\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : 1\n",
      "\n",
      "An exception occurred: No features in text. : 1\n",
      "\n",
      "An exception occurred: No features in text. : 1\n",
      "\n",
      "An exception occurred: No features in text. : 1\n",
      "\n",
      "An exception occurred: No features in text. : 1\n",
      "\n",
      "An exception occurred: No features in text. : 1\n",
      "\n",
      "An exception occurred: No features in text. : 88%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  1\n",
      "\n",
      "An exception occurred: No features in text. :  1\n",
      "\n",
      "An exception occurred: No features in text. :  1\n",
      "\n",
      "An exception occurred: No features in text. :  1\n",
      "\n",
      "An exception occurred: No features in text. :  1\n",
      "\n",
      "An exception occurred: No features in text. :  2\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 20%\n",
      "\n",
      "An exception occurred: No features in text. : 40%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. : <260 260-420 420-560 560-670 670-820 820-1060 >1060\n",
      "\n",
      "An exception occurred: No features in text. : 200\n",
      "\n",
      "An exception occurred: No features in text. : 0\n",
      "\n",
      "An exception occurred: No features in text. : 1\n",
      "\n",
      "An exception occurred: No features in text. : 2\n",
      "\n",
      "An exception occurred: No features in text. : 3\n",
      "\n",
      "An exception occurred: No features in text. : 4\n",
      "\n",
      "An exception occurred: No features in text. : 5 6\n",
      "\n",
      "An exception occurred: No features in text. : 7\n",
      "\n",
      "An exception occurred: No features in text. : 8\n",
      "\n",
      "An exception occurred: No features in text. : 9\n",
      "\n",
      "An exception occurred: No features in text. : 10\n",
      "\n",
      "An exception occurred: No features in text. : 11\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 20%\n",
      "\n",
      "An exception occurred: No features in text. : 40%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. : <1.2 1.2 - 1.4 1.4 - 1.6 1.6 - 1.8 1.8 - 1.9 1.9 - 2.0 >2.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.5\n",
      "\n",
      "An exception occurred: No features in text. : statistics@irena.org. \n",
      "\n",
      "Read text length: 5112\n",
      "Cleaned text length: 5085\n"
     ]
    }
   ],
   "source": [
    " # Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    \n",
    "    pattern = re.compile(r'.*?\\.{3}.*?$', re.MULTILINE)\n",
    "    # Initialize an empty string to store the lines\n",
    "    raw_text = ''\n",
    "    \n",
    "    head = [next(file) for _ in range(11)]\n",
    "    next(file)\n",
    "    \n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Append the current line to the string\n",
    "        \n",
    "        if not pattern.search(line) and is_english(line):\n",
    "            raw_text += line\n",
    "            \n",
    "print(f\"Read text length: {len(raw_text)}\") \n",
    "\n",
    "text = clean_text(raw_text)\n",
    "\n",
    "print(f\"Cleaned text length: {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e17194e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'File Name': 'AFG-NEPro-2022-EN', 'Year': '2022', 'Country Name': 'Afghanistan', 'Country Code': 'AFG', 'Category': 'NEPro', 'Document Title': 'Energy Profile Afghanistan IRENA', 'Publication Date': '24th August, 2022', 'Start Year': '2014', 'End Year': '2022', 'Language': 'EN'}\n"
     ]
    }
   ],
   "source": [
    "metadata = {}\n",
    "\n",
    "# Iterate through the data list\n",
    "for item in head:\n",
    "    # Split each element by ':' and strip the resulting strings\n",
    "    key, value = item.split(':')\n",
    "    key = key.strip()\n",
    "    value = value.strip()\n",
    "    \n",
    "    # Add the key-value pair to the dictionary\n",
    "    metadata[key] = value\n",
    "\n",
    "\n",
    "if 'Exists?' in metadata:\n",
    "    metadata.pop('Exists?')\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e918387c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sections from the text: 3\n"
     ]
    }
   ],
   "source": [
    "#split the text from file into section of length 2000 for token limits\n",
    "\n",
    "text_sections = split_text_spacy(2000, text)\n",
    "print (f\"The number of sections from the text: {len(text_sections)}\")\n",
    "text_length = len(text_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d21a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "wiki_entity_list = [''] * text_length\n",
    "bert_entity_list = [''] * text_length\n",
    "gpt_entity_list = [''] * text_length\n",
    "acronyms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7c317a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.837629497051239,\n",
       "  'word': 'Masdar City',\n",
       "  'start': 339,\n",
       "  'end': 350},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9379422068595886,\n",
       "  'word': 'United Arab Emirates',\n",
       "  'start': 351,\n",
       "  'end': 371},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5029942989349365,\n",
       "  'word': 'IRENA',\n",
       "  'start': 381,\n",
       "  'end': 386},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.5442724227905273,\n",
       "  'word': 'UN SDG Database',\n",
       "  'start': 437,\n",
       "  'end': 452},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9609768390655518,\n",
       "  'word': 'WHO',\n",
       "  'start': 473,\n",
       "  'end': 476},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9636392593383789,\n",
       "  'word': 'World Bank',\n",
       "  'start': 478,\n",
       "  'end': 488},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.927504301071167,\n",
       "  'word': 'IEA',\n",
       "  'start': 490,\n",
       "  'end': 493},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.7723690867424011,\n",
       "  'word': 'IRENA',\n",
       "  'start': 495,\n",
       "  'end': 500},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5234919190406799,\n",
       "  'word': 'UNSD',\n",
       "  'start': 506,\n",
       "  'end': 510},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.7503768801689148,\n",
       "  'word': 'UN World Population Prospects',\n",
       "  'start': 513,\n",
       "  'end': 543},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.6578672528266907,\n",
       "  'word': 'UNSD Energy Balances',\n",
       "  'start': 545,\n",
       "  'end': 565},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5491010546684265,\n",
       "  'word': 'UN COMTRADE',\n",
       "  'start': 573,\n",
       "  'end': 578},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.6676246523857117,\n",
       "  'word': 'World Bank World Development Indicators',\n",
       "  'start': 580,\n",
       "  'end': 620},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5642049312591553,\n",
       "  'word': 'EDGAR',\n",
       "  'start': 624,\n",
       "  'end': 627},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.5841923356056213,\n",
       "  'word': 'REN21 Global',\n",
       "  'start': 629,\n",
       "  'end': 641},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5192948579788208,\n",
       "  'word': 'Status Report',\n",
       "  'start': 642,\n",
       "  'end': 655},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.6367794275283813,\n",
       "  'word': 'IEA',\n",
       "  'start': 658,\n",
       "  'end': 660},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.7065514922142029,\n",
       "  'word': 'IRENA',\n",
       "  'start': 663,\n",
       "  'end': 666},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.6031695604324341,\n",
       "  'word': 'Joint',\n",
       "  'start': 667,\n",
       "  'end': 672},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5141783356666565,\n",
       "  'word': 'Policies and',\n",
       "  'start': 673,\n",
       "  'end': 685},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.5032649040222168,\n",
       "  'word': 'Measures',\n",
       "  'start': 694,\n",
       "  'end': 695},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5100476741790771,\n",
       "  'word': 'Database',\n",
       "  'start': 696,\n",
       "  'end': 704},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.5594721436500549,\n",
       "  'word': 'IRENA Global',\n",
       "  'start': 706,\n",
       "  'end': 718},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5299769639968872,\n",
       "  'word': 'Atlas',\n",
       "  'start': 719,\n",
       "  'end': 724},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.6983813643455505,\n",
       "  'word': 'World',\n",
       "  'start': 730,\n",
       "  'end': 735},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5131959915161133,\n",
       "  'word': 'Bank',\n",
       "  'start': 736,\n",
       "  'end': 740},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.5687824487686157,\n",
       "  'word': 'Global Solar',\n",
       "  'start': 741,\n",
       "  'end': 753},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5109257102012634,\n",
       "  'word': 'Atlas',\n",
       "  'start': 754,\n",
       "  'end': 759},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.6670831441879272,\n",
       "  'word': 'Global Wind',\n",
       "  'start': 764,\n",
       "  'end': 776},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5129252672195435,\n",
       "  'word': 'Atlas',\n",
       "  'start': 777,\n",
       "  'end': 782},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.8560970425605774,\n",
       "  'word': 'Harmonised System',\n",
       "  'start': 1064,\n",
       "  'end': 1081},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.7533373236656189,\n",
       "  'word': 'IRENA',\n",
       "  'start': 1823,\n",
       "  'end': 1828}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "66ebf24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 0\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 1\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 2\n",
      "TIME TAKEN TO EXTRACT ENTITIES from 3 section: 14.453114032745361\n"
     ]
    }
   ],
   "source": [
    "# Extract entities by sending text sections to models\n",
    "\n",
    "start_time = time.time()\n",
    "for index in range(text_length):\n",
    "    try:\n",
    "        segment = text_sections[index]\n",
    "        \n",
    "        ## WIKINEURAL BILINGUAL MODEL\n",
    "        wiki_output = query_wiki({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(wiki_output)\n",
    "        wiki_words = list(set(apply_threshold(wiki_output, 0.7)))\n",
    "        wiki_entity_list[index] = wiki_words\n",
    "        print (\"WIKI DONE\")\n",
    "\n",
    "        ## BERT BASE MODEL\n",
    "        bert_output = query_bert({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(bert_output)\n",
    "        bert_words = list(set(apply_threshold(bert_output, 0.7)))\n",
    "        bert_entity_list[index] = bert_words\n",
    "        print (\"BERT DONE\")\n",
    "\n",
    "\n",
    "        ## GPT PROMPT\n",
    "        gpt_output = query_gpt(segment)\n",
    "        gpt_entity_list[index] = gpt_output['proper_nouns']\n",
    "\n",
    "        print (\"GPT DONE\")\n",
    "\n",
    "        ## Acronyms extraction\n",
    "        acronyms.update(gpt_output['acronyms'])\n",
    "    \n",
    "        \n",
    "        print(f\"NUMBER OF PROCESSED SECTIONS: {index}\")\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing section {index}: {str(e)}\")\n",
    "        #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "        continue  # Exit the loop in case of an error\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT ENTITIES from {text_length} section: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6df46581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TES': 'Total Energy Supply', 'TJ': 'Terajoules', 'GDP': 'Gross Domestic Product', 'PPP': 'Purchasing Power Parity', 'TFEC': 'Total Final Energy Consumption', 'GW': 'Gigawatts', 'MW': 'Megawatts', 'RE': 'Renewable Energy', 'CO2': 'Carbon Dioxide', 'Elec.': 'Electricity', 'Mt': 'Metric Tons', 'AFG': 'Afghanistan', 'GWh': 'Gigawatt-hours', 'PV': 'Photovoltaic', 'MWh': 'Megawatt-hours', 'kWp': 'Kilowatt-peak', 'NREL': 'National Renewable Energy Laboratory', 'NPP': 'Net primary production', 'IRENA': 'International Renewable Energy Agency', 'UN': 'United Nations', 'SDG': 'Sustainable Development Goals', 'WHO': 'World Health Organization', 'IEA': 'International Energy Agency', 'UNSD': 'United Nations Statistics Division', 'COMTRADE': 'United Nations Commodity Trade Statistics Database', 'EDGAR': 'Emissions Database for Global Atmospheric Research', 'REN21': 'Renewable Energy Policy Network for the 21st Century'}\n"
     ]
    }
   ],
   "source": [
    "#View extarcted acronyms\n",
    "print (acronyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5cd9ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Total Energy Supply']\n",
      "['TF']\n",
      "['Total Energy Supply', 'Non-renewable', 'Renewable', 'Growth', 'Primary', 'Imports', 'Exports', 'Energy', 'Coal', 'Renewables', 'Geothermal', 'Access', \"USD'000s\", 'GDP', 'Real GDP', 'Public', 'Consumption', 'Industry', 'Households', 'Other', 'Capacity', 'Utilisation', 'Net capacity', 'Installed capacity', 'Electricity', 'Commercial heat', 'Bioenergy', 'Solar', 'Fossil fuels', 'Nuclear', 'Hydro and marine', 'Avoided emissions', 'CO2 emissions', 'Per capita electricity generation']\n",
      "--------\n",
      "[]\n",
      "['Asia World Rene', 'NREL']\n",
      "['Asia', 'World', 'Afghanistan', 'Renewable', 'Geothermal', 'Biomass', 'Solar', 'Onshore', 'NREL']\n",
      "--------\n",
      "['United Arab Emirates', 'WHO', 'Masdar City', 'IEA', 'IRENA', 'UN World Population Prospects', 'Harmonised System', 'World Bank']\n",
      "['UNSD', 'IRENA Joint Policies and Measures Database', 'IRENA Global Atlas', 'Global Wind Atlas', 'UNSD Energy Ba', 'E', 'UN COMT', 'WHO', 'World Bank World Development Indicators', 'Haised System', 'UN SDG Database', 'Masdar City United Arab Emirates', 'IEA', 'World Bank Global Solar Atlas', 'IRENA', 'UN World Population Prospects', 'World Bank']\n",
      "['Biomass', 'Masdar City', 'United Arab Emirates', 'IRENA Headquarters', 'World Bank', 'World Population Prospects', 'Energy Balances', 'World Development Indicators', 'Global Status Report', 'Joint Policies and Measures Database', 'Global Atlas', 'Global Solar Atlas', 'Global Wind Atlas']\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < text_length:\n",
    "    #print (text_sections[i])\n",
    "    print (wiki_entity_list[i])\n",
    "    print (bert_entity_list[i])\n",
    "    print (gpt_entity_list[i])\n",
    "    print (\"--------\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46f014",
   "metadata": {},
   "source": [
    "Processing the Entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "017e28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw version of entities for comparison\n",
    "raw_wiki = get_raw(wiki_entity_list)\n",
    "raw_bert = get_raw(bert_entity_list)\n",
    "raw_gpt = get_raw(gpt_entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "349b1d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of matching entities in section 0: 1\n",
      "\n",
      "['Total Energy Supply']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 1: 0\n",
      "\n",
      "[]\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 2: 3\n",
      "\n",
      "['United Arab Emirates', 'World Bank', 'Masdar City']\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "entity_objects = []\n",
    "entity_filter = []\n",
    "merged = []\n",
    "i = 0\n",
    "\n",
    "while i < len(wiki_entity_list):\n",
    "    merged = merge_extracted_entities_old(raw_wiki[i], raw_bert[i], raw_gpt[i])\n",
    "    print (f\"\\nThe number of matching entities in section {i}: {len(merged)}\\n\")\n",
    "    print (merged)\n",
    "    \n",
    "    print (\"\\n--------------\")\n",
    "    entity_filter.extend(merged)\n",
    "    entity_objects.append(merged)\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7c43b8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total Energy Supply': 'TES', 'Terajoules': 'TJ', 'Gross Domestic Product': 'GDP', 'Purchasing Power Parity': 'PPP', 'Total Final Energy Consumption': 'TFEC', 'Gigawatts': 'GW', 'Megawatts': 'MW', 'Renewable Energy': 'RE', 'Carbon Dioxide': 'CO2', 'Electricity': 'Elec.', 'Metric Tons': 'Mt', 'Afghanistan': 'AFG', 'Gigawatt-hours': 'GWh', 'Photovoltaic': 'PV', 'Megawatt-hours': 'MWh', 'Kilowatt-peak': 'kWp', 'National Renewable Energy Laboratory': 'NREL', 'Net primary production': 'NPP', 'International Renewable Energy Agency': 'IRENA', 'United Nations': 'UN', 'Sustainable Development Goals': 'SDG', 'World Health Organization': 'WHO', 'International Energy Agency': 'IEA', 'United Nations Statistics Division': 'UNSD', 'United Nations Commodity Trade Statistics Database': 'COMTRADE', 'Emissions Database for Global Atmospheric Research': 'EDGAR', 'Renewable Energy Policy Network for the 21st Century': 'REN21'}\n"
     ]
    }
   ],
   "source": [
    "# invert acronyms dict to ease look up\n",
    "acronyms_dict = {v: k for k, v in acronyms.items()}\n",
    "print (acronyms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d730afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize lists for storing entities and relations\n",
    "entities_list_unfiltered = []\n",
    "relations_list = []\n",
    "entities_with_sections = []\n",
    "relations_with_section = []\n",
    "seen_entities = set()\n",
    "seen_acronyms = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a61fea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORIZED ENTITIES of Section: 0\n",
      "\n",
      "[{'entity': 'Total Energy Supply', 'category': 'Organization', 'acronym': 'TES'}]\n",
      "\n",
      " EXTRACTED RELATIONS: \n",
      "\n",
      "[{'Subject': 'Total Energy Supply', 'Relation': 'addresses', 'Object': 'Renewable energy supply in 2019', 'Description': 'Total energy supply in 2019', 'Relevance': 8}]\n",
      "\n",
      "-------------------\n",
      "CATEGORIZED ENTITIES of Section: 1\n",
      "\n",
      "[]\n",
      "\n",
      " EXTRACTED RELATIONS: \n",
      "\n",
      "[{'Relation': 'focuses on', 'Subject': 'Electricity generation trend', 'Object': 'Energy-related CO2 emissions by sector'}, {'Relation': 'contributes to', 'Subject': 'Electricity generation trend', 'Object': 'Per capita electricity generation (kWh)'}, {'Relation': 'contributes to', 'Subject': 'Electricity generation trend', 'Object': 'Renewable share (%)'}, {'Relation': 'contributes to', 'Subject': 'Electricity generation trend', 'Object': 'Gigawatt-hours (GWh)'}, {'Relation': 'contributes to', 'Subject': 'Electricity generation trend', 'Object': 'Annual generation per unit of installed PV capacity (MWh/kWp)'}, {'Relation': 'contributes to', 'Subject': 'Electricity generation trend', 'Object': 'Solar PV: Solar resource potential has been divided into seven classes'}, {'Relation': 'contributes to', 'Subject': 'Electricity generation trend', 'Object': 'Onshore wind: Potential wind power density (W/m2) is shown in the seven classes used by NREL'}, {'Relation': 'contributes to', 'Subject': 'Electricity generation trend', 'Object': 'Biomass: Net primary production (NPP) is the amount of carbon fixed by plants and accumulated as biomass each year'}]\n",
      "\n",
      "-------------------\n",
      "CATEGORIZED ENTITIES of Section: 2\n",
      "\n",
      "[{'entity': 'United Arab Emirates', 'category': 'Location'}, {'entity': 'World Bank', 'category': 'Organization'}, {'entity': 'Masdar City', 'category': 'Location'}]\n",
      "\n",
      " EXTRACTED RELATIONS: \n",
      "\n",
      "[{'Subject': 'Biomass', 'Relation': 'accumulated as biomass each year', 'Object': 'plants', 'Description': 'Net primary production (NPP) is the amount of carbon fixed by plants and accumulated as biomass each year.', 'Relevance': 8}, {'Subject': 'Biomass', 'Relation': 'biomass productivity', 'Object': 'plants', 'Description': 'It is a basic measure of biomass productivity.', 'Relevance': 7}, {'Subject': 'United Arab Emirates', 'Relation': 'in', 'Object': 'Masdar City', 'Description': None, 'Relevance': 5}, {'Subject': 'World Bank', 'Relation': 'funds', 'Object': 'Project', 'Description': None, 'Relevance': 6}, {'Subject': 'Masdar City', 'Relation': 'in', 'Object': 'United Arab Emirates', 'Description': None, 'Relevance': 5}]\n",
      "\n",
      "-------------------\n",
      "TIME TAKEN TO EXTRACT RELATIONS FROM 3 SECTIONS: 10.360953092575073\n"
     ]
    }
   ],
   "source": [
    "#loop to categorize all extracted entities and extract relations for each text_section\n",
    "start_time = time.time()\n",
    "\n",
    "for index, uncategorized_entities in enumerate(entity_objects):\n",
    "    try:\n",
    "        entities_subset = categorize_entities(text_sections[index], uncategorized_entities, categories)\n",
    "        #print(seen_acronyms)\n",
    "\n",
    "        for item in entities_subset:\n",
    "            if item[\"entity\"] not in seen_entities and item[\"entity\"] not in seen_acronyms:\n",
    "                seen_entities.add(item[\"entity\"])\n",
    "                \n",
    "                \n",
    "                if item[\"entity\"] in acronyms_dict.keys():\n",
    "                    item[\"acronym\"] = acronyms_dict[item[\"entity\"]]\n",
    "                    seen_acronyms.add(item['acronym'])\n",
    "                    \n",
    "                elif item['entity'] in acronyms.keys():\n",
    "                    item[\"acronym\"] = item['entity']\n",
    "                    item[\"entity\"] = acronyms[item[\"entity\"]]\n",
    "                    seen_acronyms.add(item['acronym'])\n",
    "                    \n",
    "                entities_list_unfiltered.append(item)\n",
    "\n",
    "        print (\"CATEGORIZED ENTITIES of Section: \" + str(index) + \"\\n\")\n",
    "        print (entities_subset)\n",
    "        #store the categorized entities in order of lists for later processing\n",
    "\n",
    "        relations_subset = extract_ontology_relations(text_sections[index], entity_objects[index], ontology)\n",
    "\n",
    "        print (\"\\n EXTRACTED RELATIONS: \\n\")\n",
    "        print (relations_subset)\n",
    "\n",
    "        relations_list.extend(relations_subset)\n",
    "\n",
    "        print (\"\\n-------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing section {index}: {str(e)}\")\n",
    "            #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "    continue  # Exit the loop in case of an error\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT RELATIONS FROM {text_length} SECTIONS: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "874c94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = []\n",
    "for i in entities_list_unfiltered:\n",
    "    if i['entity'] in entity_filter:\n",
    "        entities_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d3de5f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No entity found with the label: Total Energy Supply\n",
      "Entity found with DBpedia URI: http://dbpedia.org/resource/United_Arab_Emirates\n",
      "Entity found with DBpedia URI: http://dbpedia.org/resource/Category:World_Bank\n",
      "No abstract or comment found for this entity.\n",
      "Entity found with DBpedia URI: http://dbpedia.org/resource/Masdar_City\n"
     ]
    }
   ],
   "source": [
    "# extract summaries from Dbpedia\n",
    "entity_summaries = extract_summaries(entities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "21e264dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of entity_names, to check for valid relations\n",
    "\n",
    "entity_names = set([item['entity'] for item in entities_list])\n",
    "entity_names.update([item['acronym'] for item in entities_list if 'acronym' in item ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d5464",
   "metadata": {},
   "source": [
    "# Write the output to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af015795",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dic = {}\n",
    "final_entities = []\n",
    "\n",
    "for i in entity_summaries:\n",
    "    entity_dic[i['entity']] = i\n",
    "    \n",
    "for i in entity_dic.values():\n",
    "    final_entities.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dae837",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_entities = json.dumps(final_entities, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecf322",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Entities/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_entities)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92563017",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_relations = []\n",
    "for i in relations_list:\n",
    "    if 'Subject' in i and i['Subject'] in entity_names and i['Object'] in entity_names and i['Relation'] in relation_labels:\n",
    "        final_relations.append(i)\n",
    "    elif 'Description' in i and 'Subject' in i and i['Subject'] in entity_dic.keys():\n",
    "        entity_dic[i['Subject']].update({'information':i['Description']})\n",
    "    elif 'Description' in i and 'Object' in i and i['Object'] in entity_dic.keys():\n",
    "        entity_dic[i['Object']].update({'information':i['Description']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_relations = json.dumps(final_relations, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Relations/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_relations)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"No. of extracted entities:\" + str(len(final_entities)))\n",
    "print (\"No. of extracted relations:\" + str(len(final_relations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert relations in the Neo4j database\n",
    "insert_relations_neo4j(final_entities, final_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06754a38",
   "metadata": {},
   "source": [
    "# Add Relations to Spreadsheet for Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3a3e0",
   "metadata": {},
   "source": [
    "Do not run this if you don't have a google api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Use the credentials from the service account key JSON file you downloaded\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('energy-moonshot-ai-97aa9045e45f.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet by its title or URL\n",
    "spreadsheet = client.open_by_url('https://docs.google.com/spreadsheets/d/1yZ-XQQs52kaI5k9MjvV_CdbgWQi-GazjHHGqQUF8gko/edit')\n",
    "\n",
    "\n",
    "# Enter relations in the first sheet\n",
    "sheet = spreadsheet.get_worksheet(0)\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "# Check if there's valid data to insert\n",
    "if final_relations:\n",
    "    # Create a list of lists where each inner list represents the values of a row\n",
    "    batch_relations = []\n",
    "    for index, row_data in enumerate(final_relations):\n",
    "        row = [index, row_data['Subject'], row_data['Relation'], row_data.get('Object', ''), \n",
    "               row_data.get('Description', ''), row_data.get('Relevance', '')]\n",
    "        \n",
    "        batch_relations.append(row)\n",
    "        index = index + 1\n",
    "\n",
    "    # Insert the data into the Google Sheet starting from row 5\n",
    "    sheet.insert_rows(batch_relations, start_row_index)\n",
    "\n",
    "    print(f\"{len(final_relations)} entries added to Google Sheet.\")\n",
    "else:\n",
    "    print(\"No data to insert.\")\n",
    "    \n",
    "    \n",
    "# Enter entities in the second sheet\n",
    "sheet = spreadsheet.get_worksheet(1)\n",
    "\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "if final_entities:\n",
    "    batch_entities = []\n",
    "    for index, row_data in enumerate(final_entities):\n",
    "        row = [index, row_data['entity'], row_data['category'], row_data.get('acronym', ''), row_data.get('summary', '')]\n",
    "        batch_entities.append(row)\n",
    "        \n",
    "        index = index + 1\n",
    "    sheet.insert_rows(batch_entities, start_row_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
